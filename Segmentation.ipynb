{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from pathlib import Path\n",
    "import re\n",
    "from skimage import measure\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
    "import matplotlib as mpl\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zabtahi6294'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh = os.getcwd()\n",
    "wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = \"/home/zabtahi6294\"\n",
    "masks_path = \"/home/zabtahi6294\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: unzip: not found\n"
     ]
    }
   ],
   "source": [
    "!unzip People-Clothing-Segmentation.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 9\n",
    "classes = {'bg':0, 'accessories': 1,  'bag': 2,  'clothes': 3, 'shoes': 4, 'glasses': 5,  'hair': 6,  'hat': 7,  'skin': 8}\n",
    "convs = {0: 0, 1: 1, 2: 2, 3: 3, 4: 3, 5: 3, 6: 3, 7: 4, 8: 3, 9: 1, 10: 3, 11: 3, 12: 4, 13: 3, 14: 3, 15: 1, 16: 4, 17: 5, 18: 3, 19: 6, 20: 7, 21: 4, 22: 3, 23: 3, 24: 3, 25: 3, 26: 3, 27: 3, 28: 4, 29: 1, 30: 3, 31: 3, 32: 4, 33: 3, 34: 1, 35: 3, 36: 4, 37: 3, 38: 3, 39: 4, 40: 3, 41: 8, 42: 3, 43: 4, 44: 3, 45: 3, 46: 3, 47: 5, 48: 3, 49: 3, 50: 3, 51: 3, 52: 3, 53: 3, 54: 3, 55: 3, 56: 3, 57: 1, 58: 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "def standardize(x):\n",
    "    x = np.array(x, dtype='float64')\n",
    "    x -= np.min(x)\n",
    "    x /= np.percentile(x, 98)\n",
    "    x[x > 1] = 1\n",
    "    return x\n",
    "\n",
    "def preprocessing(img):\n",
    "    image = np.array(img)   \n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    image = np.zeros_like(image)\n",
    "    image[:,:,0] = gray\n",
    "    image[:,:,1] = gray\n",
    "    image[:,:,2] = gray\n",
    "    image = standardize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capture training image info as a list\n",
    "images_paths = []\n",
    "masks_paths = [] \n",
    "\n",
    "for imgname in os.listdir(images_path):\n",
    "  images_paths.append(os.path.join(images_path,imgname))\n",
    "\n",
    "for imgname in os.listdir(masks_path):\n",
    "  masks_paths.append(os.path.join(masks_path,imgname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images_paths[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masks_paths[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paths.sort()\n",
    "masks_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing images, if needed\n",
    "SIZE_X = 256 \n",
    "SIZE_Y = 256\n",
    "n_classes=9 #Number of classes for segmentation\n",
    "\n",
    "#Capture training image info as a list\n",
    "train_images = []\n",
    "train_masks = [] \n",
    "\n",
    "for imgpath in tqdm.tqdm(images_paths):\n",
    "  img = cv2.imread(imgpath)\n",
    "  img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "  img = preprocessing(img)               \n",
    "  train_images.append(img)\n",
    "\n",
    "\n",
    "for maskpath in tqdm.tqdm(masks_paths):\n",
    "  mask0 = cv2.imread(maskpath, 0)\n",
    "  mask1 = cv2.resize(mask0, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
    "  for oldnum in range(0,59):\n",
    "      new_num = convs[oldnum]\n",
    "      mask1[mask1==oldnum]=new_num\n",
    "  train_masks.append(mask1)\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_masks = np.array(train_masks)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_masks, test_size = 0.10, shuffle=True, random_state = 1)\n",
    "print(\"Class values: \", np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = mpl.colors.Normalize(vmin=0, vmax=8)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for i in range(1,4):\n",
    "    plt.subplot(2,3,i)\n",
    "    img = train_images[i]\n",
    "    plt.imshow(img)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "for i in range(4,7):\n",
    "    plt.subplot(2,3,i)\n",
    "    img = np.squeeze(train_masks[i-3])\n",
    "    plt.imshow(img, cmap='jet', norm=NORM)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Model\n",
    "#Method1\n",
    "def unet_model(output_channels):\n",
    "    IMG_HEIGHT = X_train.shape[1]\n",
    "    IMG_WIDTH  = X_train.shape[2]\n",
    "    IMG_CHANNELS = X_train.shape[3]\n",
    "\n",
    "    base_model = MobileNetV2(input_shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS], include_top=False)\n",
    "\n",
    "    # Use the activations of these layers\n",
    "    layer_names = [\n",
    "        'block_1_expand_relu',   # 64x64\n",
    "        'block_3_expand_relu',   # 32x32\n",
    "        'block_6_expand_relu',   # 16x16\n",
    "        'block_13_expand_relu',  # 8x8\n",
    "        'block_16_project',      # 4x4\n",
    "    ]\n",
    "\n",
    "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    # Create the feature extraction model\n",
    "    down_stack = Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "    down_stack.trainable = False\n",
    "\n",
    "    up_stack = [\n",
    "        pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "        pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "        pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "        pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "    ]\n",
    "\n",
    "    inputs = Input(shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(inputs)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    last = Conv2DTranspose(OUTPUT_CHANNELS, 3, strides=2, padding='same')  #64x64 -> 128x128\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "\n",
    "def show_predictions(epoch, dataset=None, num=50):\n",
    "  if dataset:\n",
    "    \n",
    "    for image, mask in dataset.take(num):\n",
    "        pred_mask = model.predict(image)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.subplot(231)\n",
    "        plt.title('Testing Image')\n",
    "        plt.imshow(image[0], cmap='gray')\n",
    "        plt.subplot(232)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.imshow(mask[0], cmap='jet')\n",
    "        plt.subplot(233)\n",
    "        plt.title('Prediction on test image')\n",
    "        plt.imshow(create_mask(pred_mask), cmap='jet')\n",
    "        \n",
    "        plt.savefig(f\"results/mask_{str(ii)}.png\")\n",
    "\n",
    "        plt.show()\n",
    "  else:\n",
    "      fig = plt.figure(figsize=(12, 12))\n",
    "      fig.suptitle(f\"\\n Epoch: {str(epoch)}\\n\", fontsize=16)\n",
    "\n",
    "      plt.subplot(331)\n",
    "      plt.title('Testing Image')\n",
    "      plt.imshow(train_images[num], cmap='gray')\n",
    "      plt.subplot(332)\n",
    "      plt.title('Ground Truth')\n",
    "      plt.imshow(train_masks[num], cmap='jet')\n",
    "      plt.subplot(333)\n",
    "      plt.title('Prediction on test image')\n",
    "      plt.imshow(create_mask(model.predict(train_images[num][tf.newaxis, ...]))[:,:,0], cmap='jet')\n",
    "\n",
    "      plt.subplot(334)\n",
    "      plt.imshow(train_images[num+16], cmap='gray')\n",
    "      plt.subplot(335)\n",
    "      plt.imshow(train_masks[num+16], cmap='jet')\n",
    "      plt.subplot(336)\n",
    "      plt.imshow(create_mask(model.predict(train_images[num+16][tf.newaxis, ...]))[:,:,0], cmap='jet')\n",
    "      plt.subplot(337)\n",
    "      plt.imshow(train_images[num+14], cmap='gray')\n",
    "      plt.subplot(338)\n",
    "      plt.imshow(train_masks[num+14], cmap='jet')\n",
    "      plt.subplot(339)\n",
    "      plt.imshow(create_mask(model.predict(train_images[num+14][tf.newaxis, ...]))[:,:,0], cmap='jet')\n",
    "\n",
    "      #plt.savefig(f\"results/mask_{str(num+100)}_{str(epoch)}.png\")\n",
    "\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model(OUTPUT_CHANNELS)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])   # \n",
    "              \n",
    "              metrics=[tf.keras.metrics.MeanIoU(num_classes=9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "EPOCHS = 50\n",
    "VAL_SUBSPLITS = 5\n",
    "BATCH_SIZE = 16\n",
    "VALIDATION_STEPS = len(X_val)//BATCH_SIZE//VAL_SUBSPLITS\n",
    "STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\n",
    "# sample_image = train_images[0]\n",
    "# sample_mask = train_masks[0]\n",
    "\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        show_predictions(epoch)\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "model_history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
    "                           batch_size = BATCH_SIZE, \n",
    "                          verbose=1, \n",
    "                          #steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          #validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          callbacks=[DisplayCallback()]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/clothes_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/clothes_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Results\n",
    "test_path = \"./data/test_images\" \n",
    "\n",
    "test_paths = [] \n",
    "\n",
    "for imgname in os.listdir(test_path):\n",
    "  test_paths.append(os.path.join(test_path,imgname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./data/test_images\" \n",
    "timgnum = 0\n",
    "img_num = int(test_paths[timgnum].split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1])\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "img = cv2.imread(test_paths[timgnum])\n",
    "img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "img = preprocessing(img)\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "pred = np.array(create_mask(model.predict(img[tf.newaxis, ...])))\n",
    "plt.imshow(np.squeeze(pred))\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(train_masks[img_num-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = model_history.history\n",
    "acc=history_1['accuracy']\n",
    "val_acc = history_1['val_accuracy']\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(acc[:150], '-', label='Training')\n",
    "plt.plot(val_acc[:150], '--', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.7,1.0])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
